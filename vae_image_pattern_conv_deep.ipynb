{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-11T14:30:52.242944Z",
     "start_time": "2024-01-11T14:30:51.524350Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1018, 240)\n",
      "RGB\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from pillow_heif import register_heif_opener\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "import time\n",
    "import random\n",
    "from functools import reduce\n",
    "import operator\n",
    "\n",
    "torch.manual_seed(112916)\n",
    "\n",
    "register_heif_opener()\n",
    "\n",
    "image = Image.open('data/IMG_1432.HEIC')\n",
    "\n",
    "print(image.size)\n",
    "print(image.mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c423a108ba6e62d",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-11T14:30:53.097939Z",
     "start_time": "2024-01-11T14:30:53.084527Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 240, 1018])\n"
     ]
    }
   ],
   "source": [
    "DEVICE = 'mps'\n",
    "\n",
    "transform = transforms.Compose([ \n",
    "    transforms.PILToTensor() \n",
    "])\n",
    "\n",
    "img_tensor = transform(image)\n",
    "img_tensor = img_tensor.to(torch.float) / 255.0\n",
    "\n",
    "print(img_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acd7d8e49bfbe5a",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-11T14:30:53.766086Z",
     "start_time": "2024-01-11T14:30:53.763471Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slice Shape: (3, 240, 32)\n",
      "Slice H/W Ratio: 7\n"
     ]
    }
   ],
   "source": [
    "IMAGE_CHANNELS = 3\n",
    "SLICES = 31\n",
    "\n",
    "# calculate slice width\n",
    "slice_width = img_tensor.shape[2] // SLICES\n",
    "slice_height = img_tensor.shape[1]\n",
    "slice_shape = (IMAGE_CHANNELS, slice_height, slice_width)\n",
    "# note that we are no longer flattening the slices because our 2D inputs will go\n",
    "# to a conv layer\n",
    "\n",
    "hidden_size = 384\n",
    "latent_size = 384\n",
    "conv_1_out = 256\n",
    "conv_2_out = 384\n",
    "conv_3_out = 512\n",
    "\n",
    "relu_slope = 0.2\n",
    "\n",
    "slice_hw_ratio = slice_height//slice_width\n",
    "\n",
    "print(f'Slice Shape: {slice_shape}')\n",
    "print(f'Slice H/W Ratio: {slice_hw_ratio}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "493ff0c365cef2c4",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-11T14:30:54.680097Z",
     "start_time": "2024-01-11T14:30:54.531481Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def visualize_feature_maps(layer_output, num_feature_maps=5, cols=5):\n",
    "    \"\"\"\n",
    "    Visualizes feature maps from a given layer output.\n",
    "\n",
    "    Args:\n",
    "    - layer_output (torch.Tensor): The output tensor from the layer you wish to visualize.\n",
    "                                   Shape should be [N, C, H, W] (batch size, number of channels, height, width).\n",
    "    - num_feature_maps (int): Number of feature maps to visualize. Default is 10.\n",
    "    - cols (int): Number of columns in the subplot grid. Default is 5.\n",
    "    \"\"\"\n",
    "    # Ensure layer_output is on CPU and detached from the computation graph\n",
    "    feature_maps = layer_output[0].detach().cpu()\n",
    "\n",
    "    # Determine the number of rows needed in the subplot grid\n",
    "    rows = num_feature_maps // cols + int(num_feature_maps % cols != 0)\n",
    "\n",
    "    # Set up the figure size and grid for plots\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(cols, rows * 2.5))\n",
    "    axes = axes.flatten()  # Flatten the axes array for easy iteration\n",
    "\n",
    "    # Iterate and plot each feature map\n",
    "    for i in range(num_feature_maps):\n",
    "        if i < feature_maps.shape[0]:  # Check if the index is within the number of available feature maps\n",
    "            # Access the feature map for the ith filter and convert to numpy\n",
    "            feature_map = feature_maps[i].numpy()\n",
    "            axes[i].imshow(feature_map, cmap='gray', aspect='auto')\n",
    "            axes[i].axis('off')\n",
    "            # axes[i].set_title(f'Feature Map {i+1}')\n",
    "        else:\n",
    "            axes[i].axis('off')  # Turn off axis for any unused subplot\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96cc18b0c9e75d6e",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-11T14:30:55.574086Z",
     "start_time": "2024-01-11T14:30:55.564894Z"
    }
   },
   "outputs": [],
   "source": [
    "# encoder portion. will take our slices and (learn to) encode them into the latent space\n",
    "class TripleConvResidual(nn.Module):\n",
    "    \"\"\" Performs three convolutions with a skip connection \"\"\"\n",
    "    def __init__(self, in_dims, out_channels, kernel_size, res=True):\n",
    "        super(TripleConvResidual, self).__init__()\n",
    "        \n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = kernel_size // 2\n",
    "        self.in_dims = in_dims\n",
    "        self.in_channels = in_dims[0]\n",
    "        self.out_dims = (out_channels, in_dims[1], in_dims[2])\n",
    "        self.res = res\n",
    "\n",
    "        if res and in_dims[0] != out_channels:\n",
    "            self.resample = nn.Sequential(\n",
    "                nn.Conv2d(in_dims[0], out_channels, kernel_size=1, stride=1, bias=False),\n",
    "                nn.LayerNorm([out_channels, in_dims[1], in_dims[2]])\n",
    "            )\n",
    "        else:\n",
    "            self.resample = nn.Identity()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=self.in_channels, out_channels=out_channels, kernel_size=kernel_size, \n",
    "                               stride=1, padding=self.padding)\n",
    "        self.conv2 = nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=kernel_size, \n",
    "                               stride=1, padding=self.padding)\n",
    "        self.conv3 = nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=kernel_size, \n",
    "                               stride=1, padding=self.padding)\n",
    "        self.relu = nn.LeakyReLU(relu_slope)\n",
    "        \n",
    "        self.ln1 = nn.LayerNorm([out_channels, in_dims[1], in_dims[2]])\n",
    "        self.ln2 = nn.LayerNorm([out_channels, in_dims[1], in_dims[2]])\n",
    "        self.ln3 = nn.LayerNorm([out_channels, in_dims[1], in_dims[2]])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        res_mix = self.resample(x)\n",
    "        \n",
    "        x = self.relu(self.ln1(self.conv1(x)))\n",
    "        #print(f'conv1 out shape: {x.shape}')\n",
    "        x = self.relu(self.ln2(self.conv2(x)))\n",
    "        #print(f'conv2 out shape: {x.shape}')\n",
    "        x = self.ln3(self.conv3(x))\n",
    "\n",
    "        if self.res:\n",
    "            x = x + res_mix\n",
    "\n",
    "        x = self.relu(x)\n",
    "        #print(f'conv3 out shape: {x.shape}')\n",
    "\n",
    "        return x\n",
    "        \n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.deep_conv1 = TripleConvResidual(slice_shape, conv_1_out, 3)\n",
    "        self.deep_conv2 = TripleConvResidual(self.deep_conv1.out_dims, conv_2_out, 3)\n",
    "        self.deep_conv3 = TripleConvResidual(self.deep_conv2.out_dims, conv_3_out, 3)\n",
    "        self.conv_dims = [self.deep_conv1.out_dims, self.deep_conv2.out_dims, \n",
    "                          self.deep_conv3.out_dims]\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.pooled_dims = (self.deep_conv3.out_dims[0], \n",
    "            ((self.deep_conv3.out_dims[1] - 3) // 2) + 1,\n",
    "            ((self.deep_conv3.out_dims[2] - 3) // 2) + 1)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.flat_dims = reduce(operator.mul, self.pooled_dims)\n",
    "        #print('flat dims', self.flat_dims)\n",
    "        self.relu = nn.LeakyReLU(relu_slope)\n",
    "        self.linear1 = nn.Linear(self.flat_dims, hidden_size)\n",
    "        self.norm1 = nn.LayerNorm([self.flat_dims])\n",
    "        self.norm2 = nn.LayerNorm([hidden_size])\n",
    "        self.mean = nn.Linear(hidden_size, latent_size)\n",
    "        self.var = nn.Linear(hidden_size, latent_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.deep_conv1(x)\n",
    "        x = self.deep_conv2(x)\n",
    "        x = self.deep_conv3(x)\n",
    "        x = self.pool1(x)\n",
    "        # visualize_feature_maps(x)\n",
    "        x = self.flatten(x)\n",
    "        #print('flatten', x.shape)\n",
    "        x = self.relu(self.norm1(x))\n",
    "        x = self.relu(self.norm2(self.linear1(x)))\n",
    "        \n",
    "        mean = self.mean(x)\n",
    "        log_variance = self.var(x)\n",
    "        \n",
    "        return mean, log_variance\n",
    "\n",
    "\n",
    "class UpsampleAndConv(nn.Module):\n",
    "    \"\"\"Peforms upsampling and refinement with a skip connection\"\"\"\n",
    "    def __init__(self, pooled_dims, scale_factor, out_channels, kernel_size, stride, padding, \n",
    "                 mode='nearest', res=True):\n",
    "        super(UpsampleAndConv, self).__init__()\n",
    "        \n",
    "        self.scale_factor = scale_factor\n",
    "        in_channels = pooled_dims[0]\n",
    "        self.refine_conv1 = nn.Conv2d(in_channels, in_channels, kernel_size, stride, padding+1)\n",
    "        self.refine_conv2 = nn.Conv2d(in_channels, in_channels, kernel_size, stride, padding)\n",
    "        self.refine_conv3 = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        \n",
    "        upsample_dims = (in_channels, pooled_dims[1]*scale_factor, pooled_dims[2]*scale_factor)\n",
    "        padded_upsample_dims = (upsample_dims[0], upsample_dims[1]+padding+1, upsample_dims[2]+padding+1)\n",
    "        \n",
    "        self.ln1 = nn.LayerNorm([*padded_upsample_dims])\n",
    "        self.ln2 = nn.LayerNorm([*padded_upsample_dims])\n",
    "        self.ln3 = nn.LayerNorm([out_channels, padded_upsample_dims[1], padded_upsample_dims[2]])\n",
    "        \n",
    "        if res:\n",
    "            self.resample = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding+1, bias=False),\n",
    "                nn.LayerNorm([out_channels, padded_upsample_dims[1], padded_upsample_dims[2]])\n",
    "            )\n",
    "        else:\n",
    "            self.resample = None\n",
    "            \n",
    "        self.relu = nn.LeakyReLU(relu_slope)\n",
    "        \n",
    "        self.mode = mode\n",
    "        self.res = res\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_upsampled = F.interpolate(x, scale_factor=self.scale_factor, mode=self.mode,\n",
    "                                    align_corners=True if self.mode == 'bilinear' else None)\n",
    "        #print('x_upsampled shape', x_upsampled.shape)\n",
    "        x = self.relu(self.ln1(self.refine_conv1(x_upsampled)))\n",
    "        #print('refine_conv1 shape', x.shape)\n",
    "        x = self.relu(self.ln2(self.refine_conv2(x)))\n",
    "        #print('refine_conv2 shape', x.shape)\n",
    "        x = self.ln3(self.refine_conv3(x))\n",
    "        #print('refine_conv3 shape', x.shape)\n",
    "        \n",
    "        if self.res:\n",
    "            x = x + self.resample(x_upsampled)\n",
    "        \n",
    "        #print('refine_conv3 shape', x.shape)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# decoder portion. takes data that is normally distributed in the latent space\n",
    "# and decodes to the input space\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, encoder_conv_out_dims, encoder_flat_dims, encoder_pooled_dims):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.latent_in = nn.Linear(latent_size, hidden_size)\n",
    "        self.hidden_layer1 = nn.Linear(hidden_size, encoder_flat_dims)\n",
    "        self.unflatten = nn.Unflatten(dim=1, unflattened_size=encoder_pooled_dims)\n",
    "        self.unpool = UpsampleAndConv(mode='nearest', scale_factor=2, \n",
    "                                      pooled_dims=encoder_pooled_dims, \n",
    "                                      out_channels=encoder_conv_out_dims[1][0], \n",
    "                                      kernel_size=3, stride=1, padding=1, res=False)\n",
    "        self.unconv2 = TripleConvResidual(encoder_conv_out_dims[1], encoder_conv_out_dims[0][0], 3, res=False)\n",
    "        self.unconv1 = TripleConvResidual(encoder_conv_out_dims[0], IMAGE_CHANNELS, 3, res=False)\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm([hidden_size])\n",
    "        self.norm2 = nn.LayerNorm([encoder_flat_dims])\n",
    "        self.norm3 = nn.LayerNorm([*slice_shape])\n",
    "        \n",
    "        self.relu = nn.LeakyReLU(relu_slope) # nn.LeakyReLU(0.2)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.norm1(self.latent_in(x)))\n",
    "        x = self.relu(self.norm2(self.hidden_layer1(x)))\n",
    "        x = self.unflatten(x)\n",
    "        x = self.unpool(x)\n",
    "        #visualize_feature_maps(x)\n",
    "        #print(f'unpool shape: {x.shape}')\n",
    "        x = self.unconv2(x)\n",
    "        # visualize_feature_maps(x)\n",
    "        #print(f'uc2 shape: {x.shape}')\n",
    "        x = self.norm3(self.unconv1(x))\n",
    "        #print(f'uc1 shape: {x.shape}')\n",
    "        x = self.sigmoid(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "# puts it all together and adds the reparameterization trick to \n",
    "# establish the Gaussian distribution\n",
    "class VAETrainingModel(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(VAETrainingModel, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "    \n",
    "    def reparameterize(self, mean, var):\n",
    "        epsilon = torch.randn_like(var).to(DEVICE)\n",
    "        return mean + epsilon * var\n",
    "        \n",
    "    def forward(self, x):\n",
    "        mean, log_var = self.encoder(x)\n",
    "        x = self.reparameterize(mean, log_var)\n",
    "        x = self.decoder(x)\n",
    "        \n",
    "        return x, mean, log_var\n",
    "    \n",
    "def cost_function(x, x_pred, mean, log_var):\n",
    "    reproduction_loss = nn.functional.mse_loss(x_pred, x)\n",
    "    KLD      = - 0.5 * torch.sum(1 + log_var - mean.pow(2) - log_var.exp())\n",
    "\n",
    "    return reproduction_loss + KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d956fdc74791d99",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-11T14:30:56.652518Z",
     "start_time": "2024-01-11T14:30:56.643934Z"
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 15\n",
    "EPOCH_ITERS = img_tensor.shape[2] // slice_width // BATCH_SIZE\n",
    "MODEL_WEIGHTS_FILE = 'out/model_conv_deep.pt'\n",
    "\n",
    "# train the model on the flattened data\n",
    "def train_model(model, next_batch, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    print(f'Model is training with {num_epochs} epochs with {EPOCH_ITERS} iterations per epoch')\n",
    "    since = time.time()\n",
    "\n",
    "    best_loss = np.inf\n",
    "    best_epoch = 0\n",
    "\n",
    "    phases = ['train', 'val']\n",
    "\n",
    "    # Keep track of how loss evolves during training\n",
    "    training_curves = {}\n",
    "    for phase in phases:\n",
    "        training_curves[phase+'_loss'] = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'\\nEpoch {epoch+1}/{num_epochs}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        for phase in phases:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "\n",
    "            # iterate over enough batches to try to cover the entire image (1 epoch)\n",
    "            for _ in range(EPOCH_ITERS):\n",
    "                inputs = next_batch(BATCH_SIZE)\n",
    "                # print('Batch is', inputs.shape[0])\n",
    "                inputs = inputs.to(DEVICE)\n",
    "                # our targets are the same as our inputs\n",
    "                targets = inputs.to(DEVICE)\n",
    "                # print(inputs.shape)\n",
    "                # print(targets.shape)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    x_pred, mean, log_var = model(inputs)\n",
    "                    # print(outputs.shape)\n",
    "                    loss = criterion(targets, x_pred, mean, log_var)\n",
    "\n",
    "                    # backward + update weights only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            if scheduler and phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / BATCH_SIZE\n",
    "            training_curves[phase+'_loss'].append(epoch_loss)\n",
    "\n",
    "            print(f'{phase:5} Loss: {epoch_loss:.4f}')\n",
    "\n",
    "            # save the model if it's the best loss\n",
    "            if phase == 'val' and epoch_loss < best_loss:\n",
    "              best_epoch = epoch\n",
    "              best_loss = epoch_loss\n",
    "              with open(MODEL_WEIGHTS_FILE, 'wb') as f:\n",
    "                torch.save(model.state_dict(), f)\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'\\nTraining complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best val Loss: {best_loss:4f} at epoch {best_epoch}')\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(torch.load(MODEL_WEIGHTS_FILE, map_location=DEVICE))\n",
    "\n",
    "    return model, training_curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8e4f3ea0d3b4e11",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-11T14:30:59.750457Z",
     "start_time": "2024-01-11T14:30:57.723405Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder:  435.361536 M parameters\n",
      "Decoder:  396.005035 M parameters\n"
     ]
    }
   ],
   "source": [
    "LEARNING_RATE = 3e-4\n",
    "\n",
    "encoder = Encoder().to(DEVICE)\n",
    "print('Encoder: ', sum(p.numel() for p in encoder.parameters())/1e6, 'M parameters')\n",
    "decoder = Decoder(encoder_conv_out_dims=encoder.conv_dims, \n",
    "                  encoder_flat_dims=encoder.flat_dims,\n",
    "                  encoder_pooled_dims=encoder.pooled_dims).to(DEVICE)\n",
    "print('Decoder: ', sum(p.numel() for p in decoder.parameters())/1e6, 'M parameters')\n",
    "training_model = VAETrainingModel(encoder, decoder).to(DEVICE)\n",
    "\n",
    "optimizer_ = torch.optim.AdamW(training_model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler_ = torch.optim.lr_scheduler.ExponentialLR(optimizer_, gamma=0.999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d82f18e15364064e",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-11T14:31:00.526268Z",
     "start_time": "2024-01-11T14:31:00.523157Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_batch(batch_size):\n",
    "    \"\"\" Returns a batch consisting of batch_size slices of the training data \"\"\"\n",
    "    # our img_tensor is torch.Size([3, 1917, 8141])\n",
    "    # so we want to start at a random spot in the tensor that is < slice_width\n",
    "    # then take a slice_width chunk of the 3rd dimension which is the width\n",
    "    \n",
    "    # calculate the maximum start index for the width\n",
    "    max_start_index_width = img_tensor.size(2) - slice_width\n",
    "\n",
    "    batch_list = []\n",
    "    for _ in range(batch_size):\n",
    "        start = random.randint(0, max_start_index_width)\n",
    "        img_slice = img_tensor[:, :, start:start + slice_width]\n",
    "        #print(f'Random tensor of size {img_slice.shape} width at {start} ')\n",
    "        batch_list.append(img_slice)\n",
    "        \n",
    "    ret = torch.stack(batch_list)\n",
    "    #print(ret.shape)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55e91d6057814464",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-11T14:56:00.796365Z",
     "start_time": "2024-01-11T14:31:01.409439Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is training with 300 epochs with 2 iterations per epoch\n",
      "\n",
      "Epoch 1/300\n",
      "----------\n",
      "train Loss: 1468.7123\n",
      "val   Loss: 962.4922\n",
      "\n",
      "Epoch 2/300\n",
      "----------\n",
      "train Loss: 827.7349\n",
      "val   Loss: 478.4034\n",
      "\n",
      "Epoch 3/300\n",
      "----------\n",
      "train Loss: 413.0173\n",
      "val   Loss: 238.3999\n",
      "\n",
      "Epoch 4/300\n",
      "----------\n",
      "train Loss: 196.2464\n",
      "val   Loss: 101.6418\n",
      "\n",
      "Epoch 5/300\n",
      "----------\n",
      "train Loss: 87.9264\n",
      "val   Loss: 64.3330\n",
      "\n",
      "Epoch 6/300\n",
      "----------\n",
      "train Loss: 63.9535\n",
      "val   Loss: 58.9740\n",
      "\n",
      "Epoch 7/300\n",
      "----------\n",
      "train Loss: 60.4391\n",
      "val   Loss: 64.7548\n",
      "\n",
      "Epoch 8/300\n",
      "----------\n",
      "train Loss: 67.2509\n",
      "val   Loss: 71.3078\n",
      "\n",
      "Epoch 9/300\n",
      "----------\n",
      "train Loss: 71.3895\n",
      "val   Loss: 68.9614\n",
      "\n",
      "Epoch 10/300\n",
      "----------\n",
      "train Loss: 66.6123\n",
      "val   Loss: 58.7344\n",
      "\n",
      "Epoch 11/300\n",
      "----------\n",
      "train Loss: 55.5628\n",
      "val   Loss: 46.0263\n",
      "\n",
      "Epoch 12/300\n",
      "----------\n",
      "train Loss: 43.0971\n",
      "val   Loss: 34.7549\n",
      "\n",
      "Epoch 13/300\n",
      "----------\n",
      "train Loss: 31.9184\n",
      "val   Loss: 25.0931\n",
      "\n",
      "Epoch 14/300\n",
      "----------\n",
      "train Loss: 23.2646\n",
      "val   Loss: 18.2402\n",
      "\n",
      "Epoch 15/300\n",
      "----------\n",
      "train Loss: 16.7092\n",
      "val   Loss: 13.2515\n",
      "\n",
      "Epoch 16/300\n",
      "----------\n",
      "train Loss: 12.2818\n",
      "val   Loss: 10.3094\n",
      "\n",
      "Epoch 17/300\n",
      "----------\n",
      "train Loss: 9.9912\n",
      "val   Loss: 9.1225\n",
      "\n",
      "Epoch 18/300\n",
      "----------\n",
      "train Loss: 8.9028\n",
      "val   Loss: 8.5951\n",
      "\n",
      "Epoch 19/300\n",
      "----------\n",
      "train Loss: 8.4505\n",
      "val   Loss: 7.8576\n",
      "\n",
      "Epoch 20/300\n",
      "----------\n",
      "train Loss: 7.6510\n",
      "val   Loss: 6.8772\n",
      "\n",
      "Epoch 21/300\n",
      "----------\n",
      "train Loss: 6.5974\n",
      "val   Loss: 5.5745\n",
      "\n",
      "Epoch 22/300\n",
      "----------\n",
      "train Loss: 5.1984\n",
      "val   Loss: 4.3822\n",
      "\n",
      "Epoch 23/300\n",
      "----------\n",
      "train Loss: 4.1642\n",
      "val   Loss: 3.2825\n",
      "\n",
      "Epoch 24/300\n",
      "----------\n",
      "train Loss: 3.2372\n",
      "val   Loss: 2.7306\n",
      "\n",
      "Epoch 25/300\n",
      "----------\n",
      "train Loss: 2.5233\n",
      "val   Loss: 2.1743\n",
      "\n",
      "Epoch 26/300\n",
      "----------\n",
      "train Loss: 2.1211\n",
      "val   Loss: 1.8202\n",
      "\n",
      "Epoch 27/300\n",
      "----------\n",
      "train Loss: 1.8577\n",
      "val   Loss: 1.5415\n",
      "\n",
      "Epoch 28/300\n",
      "----------\n",
      "train Loss: 1.5480\n",
      "val   Loss: 1.2794\n",
      "\n",
      "Epoch 29/300\n",
      "----------\n",
      "train Loss: 1.2827\n",
      "val   Loss: 1.1654\n",
      "\n",
      "Epoch 30/300\n",
      "----------\n",
      "train Loss: 1.0646\n",
      "val   Loss: 0.9635\n",
      "\n",
      "Epoch 31/300\n",
      "----------\n",
      "train Loss: 0.9243\n",
      "val   Loss: 0.8190\n",
      "\n",
      "Epoch 32/300\n",
      "----------\n",
      "train Loss: 0.8271\n",
      "val   Loss: 0.7121\n",
      "\n",
      "Epoch 33/300\n",
      "----------\n",
      "train Loss: 0.6737\n",
      "val   Loss: 0.6364\n",
      "\n",
      "Epoch 34/300\n",
      "----------\n",
      "train Loss: 0.5811\n",
      "val   Loss: 0.5042\n",
      "\n",
      "Epoch 35/300\n",
      "----------\n",
      "train Loss: 0.5109\n",
      "val   Loss: 0.5114\n",
      "\n",
      "Epoch 36/300\n",
      "----------\n",
      "train Loss: 0.3763\n",
      "val   Loss: 0.4574\n",
      "\n",
      "Epoch 37/300\n",
      "----------\n",
      "train Loss: 0.4284\n",
      "val   Loss: 0.4409\n",
      "\n",
      "Epoch 38/300\n",
      "----------\n",
      "train Loss: 0.3539\n",
      "val   Loss: 0.3530\n",
      "\n",
      "Epoch 39/300\n",
      "----------\n",
      "train Loss: 0.2951\n",
      "val   Loss: 0.3320\n",
      "\n",
      "Epoch 40/300\n",
      "----------\n",
      "train Loss: 0.2544\n",
      "val   Loss: 0.2838\n",
      "\n",
      "Epoch 41/300\n",
      "----------\n",
      "train Loss: 0.2862\n",
      "val   Loss: 0.2822\n",
      "\n",
      "Epoch 42/300\n",
      "----------\n",
      "train Loss: 0.2570\n",
      "val   Loss: 0.2182\n",
      "\n",
      "Epoch 43/300\n",
      "----------\n",
      "train Loss: 0.2423\n",
      "val   Loss: 0.2108\n",
      "\n",
      "Epoch 44/300\n",
      "----------\n",
      "train Loss: 0.2019\n",
      "val   Loss: 0.1906\n",
      "\n",
      "Epoch 45/300\n",
      "----------\n",
      "train Loss: 0.1834\n",
      "val   Loss: 0.1957\n",
      "\n",
      "Epoch 46/300\n",
      "----------\n",
      "train Loss: 0.2084\n",
      "val   Loss: 0.1359\n",
      "\n",
      "Epoch 47/300\n",
      "----------\n",
      "train Loss: 0.1879\n",
      "val   Loss: 0.1391\n",
      "\n",
      "Epoch 48/300\n",
      "----------\n",
      "train Loss: 0.1775\n",
      "val   Loss: 0.1851\n",
      "\n",
      "Epoch 49/300\n",
      "----------\n",
      "train Loss: 0.2520\n",
      "val   Loss: 0.1931\n",
      "\n",
      "Epoch 50/300\n",
      "----------\n",
      "train Loss: 0.1791\n",
      "val   Loss: 0.1561\n",
      "\n",
      "Epoch 51/300\n",
      "----------\n",
      "train Loss: 0.1568\n",
      "val   Loss: 0.2037\n",
      "\n",
      "Epoch 52/300\n",
      "----------\n",
      "train Loss: 0.2164\n",
      "val   Loss: 0.1332\n",
      "\n",
      "Epoch 53/300\n",
      "----------\n",
      "train Loss: 0.1497\n",
      "val   Loss: 0.1663\n",
      "\n",
      "Epoch 54/300\n",
      "----------\n",
      "train Loss: 0.1737\n",
      "val   Loss: 0.1623\n",
      "\n",
      "Epoch 55/300\n",
      "----------\n",
      "train Loss: 0.1614\n",
      "val   Loss: 0.1724\n",
      "\n",
      "Epoch 56/300\n",
      "----------\n",
      "train Loss: 0.1504\n",
      "val   Loss: 0.1463\n",
      "\n",
      "Epoch 57/300\n",
      "----------\n",
      "train Loss: 0.1554\n",
      "val   Loss: 0.1442\n",
      "\n",
      "Epoch 58/300\n",
      "----------\n",
      "train Loss: 0.1645\n",
      "val   Loss: 0.1236\n",
      "\n",
      "Epoch 59/300\n",
      "----------\n",
      "train Loss: 0.1565\n",
      "val   Loss: 0.1462\n",
      "\n",
      "Epoch 60/300\n",
      "----------\n",
      "train Loss: 0.1215\n",
      "val   Loss: 0.1087\n",
      "\n",
      "Epoch 61/300\n",
      "----------\n",
      "train Loss: 0.1545\n",
      "val   Loss: 0.1597\n",
      "\n",
      "Epoch 62/300\n",
      "----------\n",
      "train Loss: 0.1447\n",
      "val   Loss: 0.1579\n",
      "\n",
      "Epoch 63/300\n",
      "----------\n",
      "train Loss: 0.1094\n",
      "val   Loss: 0.1060\n",
      "\n",
      "Epoch 64/300\n",
      "----------\n",
      "train Loss: 0.1409\n",
      "val   Loss: 0.0929\n",
      "\n",
      "Epoch 65/300\n",
      "----------\n",
      "train Loss: 0.1245\n",
      "val   Loss: 0.1257\n",
      "\n",
      "Epoch 66/300\n",
      "----------\n",
      "train Loss: 0.1393\n",
      "val   Loss: 0.1729\n",
      "\n",
      "Epoch 67/300\n",
      "----------\n",
      "train Loss: 0.1499\n",
      "val   Loss: 0.0851\n",
      "\n",
      "Epoch 68/300\n",
      "----------\n",
      "train Loss: 0.1191\n",
      "val   Loss: 0.1206\n",
      "\n",
      "Epoch 69/300\n",
      "----------\n",
      "train Loss: 0.1161\n",
      "val   Loss: 0.0997\n",
      "\n",
      "Epoch 70/300\n",
      "----------\n",
      "train Loss: 0.0957\n",
      "val   Loss: 0.0954\n",
      "\n",
      "Epoch 71/300\n",
      "----------\n",
      "train Loss: 0.1080\n",
      "val   Loss: 0.0966\n",
      "\n",
      "Epoch 72/300\n",
      "----------\n",
      "train Loss: 0.1593\n",
      "val   Loss: 0.1273\n",
      "\n",
      "Epoch 73/300\n",
      "----------\n",
      "train Loss: 0.1130\n",
      "val   Loss: 0.1056\n",
      "\n",
      "Epoch 74/300\n",
      "----------\n",
      "train Loss: 0.1125\n",
      "val   Loss: 0.0927\n",
      "\n",
      "Epoch 75/300\n",
      "----------\n",
      "train Loss: 0.1128\n",
      "val   Loss: 0.1053\n",
      "\n",
      "Epoch 76/300\n",
      "----------\n",
      "train Loss: 0.1095\n",
      "val   Loss: 0.1339\n",
      "\n",
      "Epoch 77/300\n",
      "----------\n",
      "train Loss: 0.1065\n",
      "val   Loss: 0.1040\n",
      "\n",
      "Epoch 78/300\n",
      "----------\n",
      "train Loss: 0.1595\n",
      "val   Loss: 0.1084\n",
      "\n",
      "Epoch 79/300\n",
      "----------\n",
      "train Loss: 0.1327\n",
      "val   Loss: 0.1137\n",
      "\n",
      "Epoch 80/300\n",
      "----------\n",
      "train Loss: 0.1065\n",
      "val   Loss: 0.1066\n",
      "\n",
      "Epoch 81/300\n",
      "----------\n",
      "train Loss: 0.1146\n",
      "val   Loss: 0.0820\n",
      "\n",
      "Epoch 82/300\n",
      "----------\n",
      "train Loss: 0.1494\n",
      "val   Loss: 0.1385\n",
      "\n",
      "Epoch 83/300\n",
      "----------\n",
      "train Loss: 0.1036\n",
      "val   Loss: 0.0985\n",
      "\n",
      "Epoch 84/300\n",
      "----------\n",
      "train Loss: 0.0958\n",
      "val   Loss: 0.0912\n",
      "\n",
      "Epoch 85/300\n",
      "----------\n",
      "train Loss: 0.1253\n",
      "val   Loss: 0.1358\n",
      "\n",
      "Epoch 86/300\n",
      "----------\n",
      "train Loss: 0.1283\n",
      "val   Loss: 0.1099\n",
      "\n",
      "Epoch 87/300\n",
      "----------\n",
      "train Loss: 0.1080\n",
      "val   Loss: 0.0989\n",
      "\n",
      "Epoch 88/300\n",
      "----------\n",
      "train Loss: 0.1064\n",
      "val   Loss: 0.0985\n",
      "\n",
      "Epoch 89/300\n",
      "----------\n",
      "train Loss: 0.1002\n",
      "val   Loss: 0.1031\n",
      "\n",
      "Epoch 90/300\n",
      "----------\n",
      "train Loss: 0.0875\n",
      "val   Loss: 0.1012\n",
      "\n",
      "Epoch 91/300\n",
      "----------\n",
      "train Loss: 0.0827\n",
      "val   Loss: 0.0726\n",
      "\n",
      "Epoch 92/300\n",
      "----------\n",
      "train Loss: 0.0568\n",
      "val   Loss: 0.1016\n",
      "\n",
      "Epoch 93/300\n",
      "----------\n",
      "train Loss: 0.0959\n",
      "val   Loss: 0.1033\n",
      "\n",
      "Epoch 94/300\n",
      "----------\n",
      "train Loss: 0.0893\n",
      "val   Loss: 0.0824\n",
      "\n",
      "Epoch 95/300\n",
      "----------\n",
      "train Loss: 0.0917\n",
      "val   Loss: 0.0929\n",
      "\n",
      "Epoch 96/300\n",
      "----------\n",
      "train Loss: 0.0801\n",
      "val   Loss: 0.0929\n",
      "\n",
      "Epoch 97/300\n",
      "----------\n",
      "train Loss: 0.1191\n",
      "val   Loss: 0.0992\n",
      "\n",
      "Epoch 98/300\n",
      "----------\n",
      "train Loss: 0.0778\n",
      "val   Loss: 0.0873\n",
      "\n",
      "Epoch 99/300\n",
      "----------\n",
      "train Loss: 0.1016\n",
      "val   Loss: 0.0641\n",
      "\n",
      "Epoch 100/300\n",
      "----------\n",
      "train Loss: 0.1046\n",
      "val   Loss: 0.0871\n",
      "\n",
      "Epoch 101/300\n",
      "----------\n",
      "train Loss: 0.0681\n",
      "val   Loss: 0.0831\n",
      "\n",
      "Epoch 102/300\n",
      "----------\n",
      "train Loss: 0.1007\n",
      "val   Loss: 0.0832\n",
      "\n",
      "Epoch 103/300\n",
      "----------\n",
      "train Loss: 0.0736\n",
      "val   Loss: 0.0696\n",
      "\n",
      "Epoch 104/300\n",
      "----------\n",
      "train Loss: 0.0712\n",
      "val   Loss: 0.0991\n",
      "\n",
      "Epoch 105/300\n",
      "----------\n",
      "train Loss: 0.0680\n",
      "val   Loss: 0.0743\n",
      "\n",
      "Epoch 106/300\n",
      "----------\n",
      "train Loss: 0.0910\n",
      "val   Loss: 0.0790\n",
      "\n",
      "Epoch 107/300\n",
      "----------\n",
      "train Loss: 0.0871\n",
      "val   Loss: 0.0711\n",
      "\n",
      "Epoch 108/300\n",
      "----------\n",
      "train Loss: 0.0690\n",
      "val   Loss: 0.0886\n",
      "\n",
      "Epoch 109/300\n",
      "----------\n",
      "train Loss: 0.0884\n",
      "val   Loss: 0.0527\n",
      "\n",
      "Epoch 110/300\n",
      "----------\n",
      "train Loss: 0.0690\n",
      "val   Loss: 0.0839\n",
      "\n",
      "Epoch 111/300\n",
      "----------\n",
      "train Loss: 0.0625\n",
      "val   Loss: 0.0848\n",
      "\n",
      "Epoch 112/300\n",
      "----------\n",
      "train Loss: 0.0667\n",
      "val   Loss: 0.0673\n",
      "\n",
      "Epoch 113/300\n",
      "----------\n",
      "train Loss: 0.0815\n",
      "val   Loss: 0.0836\n",
      "\n",
      "Epoch 114/300\n",
      "----------\n",
      "train Loss: 0.0614\n",
      "val   Loss: 0.0832\n",
      "\n",
      "Epoch 115/300\n",
      "----------\n",
      "train Loss: 0.0942\n",
      "val   Loss: 0.0630\n",
      "\n",
      "Epoch 116/300\n",
      "----------\n",
      "train Loss: 0.0817\n",
      "val   Loss: 0.0953\n",
      "\n",
      "Epoch 117/300\n",
      "----------\n",
      "train Loss: 0.0647\n",
      "val   Loss: 0.0644\n",
      "\n",
      "Epoch 118/300\n",
      "----------\n",
      "train Loss: 0.0537\n",
      "val   Loss: 0.0834\n",
      "\n",
      "Epoch 119/300\n",
      "----------\n",
      "train Loss: 0.0595\n",
      "val   Loss: 0.0585\n",
      "\n",
      "Epoch 120/300\n",
      "----------\n",
      "train Loss: 0.0547\n",
      "val   Loss: 0.0807\n",
      "\n",
      "Epoch 121/300\n",
      "----------\n",
      "train Loss: 0.0766\n",
      "val   Loss: 0.0712\n",
      "\n",
      "Epoch 122/300\n",
      "----------\n",
      "train Loss: 0.0702\n",
      "val   Loss: 0.0593\n",
      "\n",
      "Epoch 123/300\n",
      "----------\n",
      "train Loss: 0.0727\n",
      "val   Loss: 0.0670\n",
      "\n",
      "Epoch 124/300\n",
      "----------\n",
      "train Loss: 0.0550\n",
      "val   Loss: 0.0716\n",
      "\n",
      "Epoch 125/300\n",
      "----------\n",
      "train Loss: 0.0596\n",
      "val   Loss: 0.0681\n",
      "\n",
      "Epoch 126/300\n",
      "----------\n",
      "train Loss: 0.0915\n",
      "val   Loss: 0.0676\n",
      "\n",
      "Epoch 127/300\n",
      "----------\n",
      "train Loss: 0.0726\n",
      "val   Loss: 0.0771\n",
      "\n",
      "Epoch 128/300\n",
      "----------\n",
      "train Loss: 0.0542\n",
      "val   Loss: 0.0601\n",
      "\n",
      "Epoch 129/300\n",
      "----------\n",
      "train Loss: 0.0642\n",
      "val   Loss: 0.0873\n",
      "\n",
      "Epoch 130/300\n",
      "----------\n",
      "train Loss: 0.0587\n",
      "val   Loss: 0.0608\n",
      "\n",
      "Epoch 131/300\n",
      "----------\n",
      "train Loss: 0.0544\n",
      "val   Loss: 0.0705\n",
      "\n",
      "Epoch 132/300\n",
      "----------\n",
      "train Loss: 0.0765\n",
      "val   Loss: 0.0829\n",
      "\n",
      "Epoch 133/300\n",
      "----------\n",
      "train Loss: 0.0699\n",
      "val   Loss: 0.0603\n",
      "\n",
      "Epoch 134/300\n",
      "----------\n",
      "train Loss: 0.0585\n",
      "val   Loss: 0.0710\n",
      "\n",
      "Epoch 135/300\n",
      "----------\n",
      "train Loss: 0.0993\n",
      "val   Loss: 0.0652\n",
      "\n",
      "Epoch 136/300\n",
      "----------\n",
      "train Loss: 0.0782\n",
      "val   Loss: 0.0640\n",
      "\n",
      "Epoch 137/300\n",
      "----------\n",
      "train Loss: 0.0693\n",
      "val   Loss: 0.0524\n",
      "\n",
      "Epoch 138/300\n",
      "----------\n",
      "train Loss: 0.0554\n",
      "val   Loss: 0.0588\n",
      "\n",
      "Epoch 139/300\n",
      "----------\n",
      "train Loss: 0.0677\n",
      "val   Loss: 0.0529\n",
      "\n",
      "Epoch 140/300\n",
      "----------\n",
      "train Loss: 0.0615\n",
      "val   Loss: 0.0547\n",
      "\n",
      "Epoch 141/300\n",
      "----------\n",
      "train Loss: 0.0533\n",
      "val   Loss: 0.0600\n",
      "\n",
      "Epoch 142/300\n",
      "----------\n",
      "train Loss: 0.0556\n",
      "val   Loss: 0.0554\n",
      "\n",
      "Epoch 143/300\n",
      "----------\n",
      "train Loss: 0.0632\n",
      "val   Loss: 0.0508\n",
      "\n",
      "Epoch 144/300\n",
      "----------\n",
      "train Loss: 0.0607\n",
      "val   Loss: 0.0603\n",
      "\n",
      "Epoch 145/300\n",
      "----------\n",
      "train Loss: 0.0631\n",
      "val   Loss: 0.0575\n",
      "\n",
      "Epoch 146/300\n",
      "----------\n",
      "train Loss: 0.0630\n",
      "val   Loss: 0.0614\n",
      "\n",
      "Epoch 147/300\n",
      "----------\n",
      "train Loss: 0.0928\n",
      "val   Loss: 0.0630\n",
      "\n",
      "Epoch 148/300\n",
      "----------\n",
      "train Loss: 0.0713\n",
      "val   Loss: 0.0675\n",
      "\n",
      "Epoch 149/300\n",
      "----------\n",
      "train Loss: 0.0522\n",
      "val   Loss: 0.0427\n",
      "\n",
      "Epoch 150/300\n",
      "----------\n",
      "train Loss: 0.0587\n",
      "val   Loss: 0.0552\n",
      "\n",
      "Epoch 151/300\n",
      "----------\n",
      "train Loss: 0.0416\n",
      "val   Loss: 0.0627\n",
      "\n",
      "Epoch 152/300\n",
      "----------\n",
      "train Loss: 0.0483\n",
      "val   Loss: 0.0514\n",
      "\n",
      "Epoch 153/300\n",
      "----------\n",
      "train Loss: 0.0554\n",
      "val   Loss: 0.0627\n",
      "\n",
      "Epoch 154/300\n",
      "----------\n",
      "train Loss: 0.0540\n",
      "val   Loss: 0.0567\n",
      "\n",
      "Epoch 155/300\n",
      "----------\n",
      "train Loss: 0.0610\n",
      "val   Loss: 0.0614\n",
      "\n",
      "Epoch 156/300\n",
      "----------\n",
      "train Loss: 0.0600\n",
      "val   Loss: 0.0575\n",
      "\n",
      "Epoch 157/300\n",
      "----------\n",
      "train Loss: 0.0615\n",
      "val   Loss: 0.0651\n",
      "\n",
      "Epoch 158/300\n",
      "----------\n",
      "train Loss: 0.0636\n",
      "val   Loss: 0.0451\n",
      "\n",
      "Epoch 159/300\n",
      "----------\n",
      "train Loss: 0.0524\n",
      "val   Loss: 0.0598\n",
      "\n",
      "Epoch 160/300\n",
      "----------\n",
      "train Loss: 0.0500\n",
      "val   Loss: 0.0380\n",
      "\n",
      "Epoch 161/300\n",
      "----------\n",
      "train Loss: 0.0572\n",
      "val   Loss: 0.0425\n",
      "\n",
      "Epoch 162/300\n",
      "----------\n",
      "train Loss: 0.0542\n",
      "val   Loss: 0.0629\n",
      "\n",
      "Epoch 163/300\n",
      "----------\n",
      "train Loss: 0.0402\n",
      "val   Loss: 0.0412\n",
      "\n",
      "Epoch 164/300\n",
      "----------\n",
      "train Loss: 0.0583\n",
      "val   Loss: 0.0502\n",
      "\n",
      "Epoch 165/300\n",
      "----------\n",
      "train Loss: 0.0529\n",
      "val   Loss: 0.0510\n",
      "\n",
      "Epoch 166/300\n",
      "----------\n",
      "train Loss: 0.0488\n",
      "val   Loss: 0.0658\n",
      "\n",
      "Epoch 167/300\n",
      "----------\n",
      "train Loss: 0.0677\n",
      "val   Loss: 0.0521\n",
      "\n",
      "Epoch 168/300\n",
      "----------\n",
      "train Loss: 0.0650\n",
      "val   Loss: 0.0563\n",
      "\n",
      "Epoch 169/300\n",
      "----------\n",
      "train Loss: 0.0510\n",
      "val   Loss: 0.0491\n",
      "\n",
      "Epoch 170/300\n",
      "----------\n",
      "train Loss: 0.0551\n",
      "val   Loss: 0.0506\n",
      "\n",
      "Epoch 171/300\n",
      "----------\n",
      "train Loss: 0.0406\n",
      "val   Loss: 0.0468\n",
      "\n",
      "Epoch 172/300\n",
      "----------\n",
      "train Loss: 0.0551\n",
      "val   Loss: 0.0401\n",
      "\n",
      "Epoch 173/300\n",
      "----------\n",
      "train Loss: 0.0540\n",
      "val   Loss: 0.0385\n",
      "\n",
      "Epoch 174/300\n",
      "----------\n",
      "train Loss: 0.0446\n",
      "val   Loss: 0.0533\n",
      "\n",
      "Epoch 175/300\n",
      "----------\n",
      "train Loss: 0.0485\n",
      "val   Loss: 0.0399\n",
      "\n",
      "Epoch 176/300\n",
      "----------\n",
      "train Loss: 0.0468\n",
      "val   Loss: 0.0481\n",
      "\n",
      "Epoch 177/300\n",
      "----------\n",
      "train Loss: 0.0460\n",
      "val   Loss: 0.0395\n",
      "\n",
      "Epoch 178/300\n",
      "----------\n",
      "train Loss: 0.0526\n",
      "val   Loss: 0.0436\n",
      "\n",
      "Epoch 179/300\n",
      "----------\n",
      "train Loss: 0.0405\n",
      "val   Loss: 0.0531\n",
      "\n",
      "Epoch 180/300\n",
      "----------\n",
      "train Loss: 0.0482\n",
      "val   Loss: 0.0443\n",
      "\n",
      "Epoch 181/300\n",
      "----------\n",
      "train Loss: 0.0406\n",
      "val   Loss: 0.0392\n",
      "\n",
      "Epoch 182/300\n",
      "----------\n",
      "train Loss: 0.0543\n",
      "val   Loss: 0.0444\n",
      "\n",
      "Epoch 183/300\n",
      "----------\n",
      "train Loss: 0.0433\n",
      "val   Loss: 0.0505\n",
      "\n",
      "Epoch 184/300\n",
      "----------\n",
      "train Loss: 0.0453\n",
      "val   Loss: 0.0502\n",
      "\n",
      "Epoch 185/300\n",
      "----------\n",
      "train Loss: 0.0436\n",
      "val   Loss: 0.0408\n",
      "\n",
      "Epoch 186/300\n",
      "----------\n",
      "train Loss: 0.0448\n",
      "val   Loss: 0.0429\n",
      "\n",
      "Epoch 187/300\n",
      "----------\n",
      "train Loss: 0.0482\n",
      "val   Loss: 0.0361\n",
      "\n",
      "Epoch 188/300\n",
      "----------\n",
      "train Loss: 0.0372\n",
      "val   Loss: 0.0523\n",
      "\n",
      "Epoch 189/300\n",
      "----------\n",
      "train Loss: 0.0441\n",
      "val   Loss: 0.0396\n",
      "\n",
      "Epoch 190/300\n",
      "----------\n",
      "train Loss: 0.0562\n",
      "val   Loss: 0.0368\n",
      "\n",
      "Epoch 191/300\n",
      "----------\n",
      "train Loss: 0.0396\n",
      "val   Loss: 0.0400\n",
      "\n",
      "Epoch 192/300\n",
      "----------\n",
      "train Loss: 0.0533\n",
      "val   Loss: 0.0515\n",
      "\n",
      "Epoch 193/300\n",
      "----------\n",
      "train Loss: 0.0474\n",
      "val   Loss: 0.0390\n",
      "\n",
      "Epoch 194/300\n",
      "----------\n",
      "train Loss: 0.0442\n",
      "val   Loss: 0.0606\n",
      "\n",
      "Epoch 195/300\n",
      "----------\n",
      "train Loss: 0.0389\n",
      "val   Loss: 0.0446\n",
      "\n",
      "Epoch 196/300\n",
      "----------\n",
      "train Loss: 0.0444\n",
      "val   Loss: 0.0462\n",
      "\n",
      "Epoch 197/300\n",
      "----------\n",
      "train Loss: 0.0446\n",
      "val   Loss: 0.0509\n",
      "\n",
      "Epoch 198/300\n",
      "----------\n",
      "train Loss: 0.0450\n",
      "val   Loss: 0.0456\n",
      "\n",
      "Epoch 199/300\n",
      "----------\n",
      "train Loss: 0.0409\n",
      "val   Loss: 0.0491\n",
      "\n",
      "Epoch 200/300\n",
      "----------\n",
      "train Loss: 0.0428\n",
      "val   Loss: 0.0391\n",
      "\n",
      "Epoch 201/300\n",
      "----------\n",
      "train Loss: 0.0422\n",
      "val   Loss: 0.0381\n",
      "\n",
      "Epoch 202/300\n",
      "----------\n",
      "train Loss: 0.0370\n",
      "val   Loss: 0.0348\n",
      "\n",
      "Epoch 203/300\n",
      "----------\n",
      "train Loss: 0.0366\n",
      "val   Loss: 0.0523\n",
      "\n",
      "Epoch 204/300\n",
      "----------\n",
      "train Loss: 0.0353\n",
      "val   Loss: 0.0415\n",
      "\n",
      "Epoch 205/300\n",
      "----------\n",
      "train Loss: 0.0369\n",
      "val   Loss: 0.0310\n",
      "\n",
      "Epoch 206/300\n",
      "----------\n",
      "train Loss: 0.0357\n",
      "val   Loss: 0.0410\n",
      "\n",
      "Epoch 207/300\n",
      "----------\n",
      "train Loss: 0.0375\n",
      "val   Loss: 0.0397\n",
      "\n",
      "Epoch 208/300\n",
      "----------\n",
      "train Loss: 0.0369\n",
      "val   Loss: 0.0419\n",
      "\n",
      "Epoch 209/300\n",
      "----------\n",
      "train Loss: 0.0374\n",
      "val   Loss: 0.0351\n",
      "\n",
      "Epoch 210/300\n",
      "----------\n",
      "train Loss: 0.0412\n",
      "val   Loss: 0.0323\n",
      "\n",
      "Epoch 211/300\n",
      "----------\n",
      "train Loss: 0.0370\n",
      "val   Loss: 0.0402\n",
      "\n",
      "Epoch 212/300\n",
      "----------\n",
      "train Loss: 0.0309\n",
      "val   Loss: 0.0503\n",
      "\n",
      "Epoch 213/300\n",
      "----------\n",
      "train Loss: 0.0369\n",
      "val   Loss: 0.0377\n",
      "\n",
      "Epoch 214/300\n",
      "----------\n",
      "train Loss: 0.0280\n",
      "val   Loss: 0.0374\n",
      "\n",
      "Epoch 215/300\n",
      "----------\n",
      "train Loss: 0.0291\n",
      "val   Loss: 0.0446\n",
      "\n",
      "Epoch 216/300\n",
      "----------\n",
      "train Loss: 0.0399\n",
      "val   Loss: 0.0373\n",
      "\n",
      "Epoch 217/300\n",
      "----------\n",
      "train Loss: 0.0404\n",
      "val   Loss: 0.0408\n",
      "\n",
      "Epoch 218/300\n",
      "----------\n",
      "train Loss: 0.0354\n",
      "val   Loss: 0.0337\n",
      "\n",
      "Epoch 219/300\n",
      "----------\n",
      "train Loss: 0.0342\n",
      "val   Loss: 0.0331\n",
      "\n",
      "Epoch 220/300\n",
      "----------\n",
      "train Loss: 0.0404\n",
      "val   Loss: 0.0356\n",
      "\n",
      "Epoch 221/300\n",
      "----------\n",
      "train Loss: 0.0352\n",
      "val   Loss: 0.0357\n",
      "\n",
      "Epoch 222/300\n",
      "----------\n",
      "train Loss: 0.0394\n",
      "val   Loss: 0.0384\n",
      "\n",
      "Epoch 223/300\n",
      "----------\n",
      "train Loss: 0.0344\n",
      "val   Loss: 0.0387\n",
      "\n",
      "Epoch 224/300\n",
      "----------\n",
      "train Loss: 0.0323\n",
      "val   Loss: 0.0356\n",
      "\n",
      "Epoch 225/300\n",
      "----------\n",
      "train Loss: 0.0328\n",
      "val   Loss: 0.0292\n",
      "\n",
      "Epoch 226/300\n",
      "----------\n",
      "train Loss: 0.0275\n",
      "val   Loss: 0.0375\n",
      "\n",
      "Epoch 227/300\n",
      "----------\n",
      "train Loss: 0.0318\n",
      "val   Loss: 0.0329\n",
      "\n",
      "Epoch 228/300\n",
      "----------\n",
      "train Loss: 0.0347\n",
      "val   Loss: 0.0356\n",
      "\n",
      "Epoch 229/300\n",
      "----------\n",
      "train Loss: 0.0353\n",
      "val   Loss: 0.0353\n",
      "\n",
      "Epoch 230/300\n",
      "----------\n",
      "train Loss: 0.0306\n",
      "val   Loss: 0.0414\n",
      "\n",
      "Epoch 231/300\n",
      "----------\n",
      "train Loss: 0.0417\n",
      "val   Loss: 0.0346\n",
      "\n",
      "Epoch 232/300\n",
      "----------\n",
      "train Loss: 0.0335\n",
      "val   Loss: 0.0301\n",
      "\n",
      "Epoch 233/300\n",
      "----------\n",
      "train Loss: 0.0356\n",
      "val   Loss: 0.0311\n",
      "\n",
      "Epoch 234/300\n",
      "----------\n",
      "train Loss: 0.0352\n",
      "val   Loss: 0.0286\n",
      "\n",
      "Epoch 235/300\n",
      "----------\n",
      "train Loss: 0.0350\n",
      "val   Loss: 0.0302\n",
      "\n",
      "Epoch 236/300\n",
      "----------\n",
      "train Loss: 0.0334\n",
      "val   Loss: 0.0330\n",
      "\n",
      "Epoch 237/300\n",
      "----------\n",
      "train Loss: 0.0362\n",
      "val   Loss: 0.0371\n",
      "\n",
      "Epoch 238/300\n",
      "----------\n",
      "train Loss: 0.0339\n",
      "val   Loss: 0.0352\n",
      "\n",
      "Epoch 239/300\n",
      "----------\n",
      "train Loss: 0.0335\n",
      "val   Loss: 0.0353\n",
      "\n",
      "Epoch 240/300\n",
      "----------\n",
      "train Loss: 0.0325\n",
      "val   Loss: 0.0272\n",
      "\n",
      "Epoch 241/300\n",
      "----------\n",
      "train Loss: 0.0376\n",
      "val   Loss: 0.0337\n",
      "\n",
      "Epoch 242/300\n",
      "----------\n",
      "train Loss: 0.0299\n",
      "val   Loss: 0.0289\n",
      "\n",
      "Epoch 243/300\n",
      "----------\n",
      "train Loss: 0.0279\n",
      "val   Loss: 0.0315\n",
      "\n",
      "Epoch 244/300\n",
      "----------\n",
      "train Loss: 0.0338\n",
      "val   Loss: 0.0288\n",
      "\n",
      "Epoch 245/300\n",
      "----------\n",
      "train Loss: 0.0334\n",
      "val   Loss: 0.0307\n",
      "\n",
      "Epoch 246/300\n",
      "----------\n",
      "train Loss: 0.0348\n",
      "val   Loss: 0.0283\n",
      "\n",
      "Epoch 247/300\n",
      "----------\n",
      "train Loss: 0.0333\n",
      "val   Loss: 0.0306\n",
      "\n",
      "Epoch 248/300\n",
      "----------\n",
      "train Loss: 0.0319\n",
      "val   Loss: 0.0295\n",
      "\n",
      "Epoch 249/300\n",
      "----------\n",
      "train Loss: 0.0296\n",
      "val   Loss: 0.0258\n",
      "\n",
      "Epoch 250/300\n",
      "----------\n",
      "train Loss: 0.0346\n",
      "val   Loss: 0.0329\n",
      "\n",
      "Epoch 251/300\n",
      "----------\n",
      "train Loss: 0.0327\n",
      "val   Loss: 0.0298\n",
      "\n",
      "Epoch 252/300\n",
      "----------\n",
      "train Loss: 0.0283\n",
      "val   Loss: 0.0364\n",
      "\n",
      "Epoch 253/300\n",
      "----------\n",
      "train Loss: 0.0309\n",
      "val   Loss: 0.0294\n",
      "\n",
      "Epoch 254/300\n",
      "----------\n",
      "train Loss: 0.0305\n",
      "val   Loss: 0.0304\n",
      "\n",
      "Epoch 255/300\n",
      "----------\n",
      "train Loss: 0.0294\n",
      "val   Loss: 0.0323\n",
      "\n",
      "Epoch 256/300\n",
      "----------\n",
      "train Loss: 0.0280\n",
      "val   Loss: 0.0265\n",
      "\n",
      "Epoch 257/300\n",
      "----------\n",
      "train Loss: 0.0325\n",
      "val   Loss: 0.0299\n",
      "\n",
      "Epoch 258/300\n",
      "----------\n",
      "train Loss: 0.0273\n",
      "val   Loss: 0.0311\n",
      "\n",
      "Epoch 259/300\n",
      "----------\n",
      "train Loss: 0.0306\n",
      "val   Loss: 0.0286\n",
      "\n",
      "Epoch 260/300\n",
      "----------\n",
      "train Loss: 0.0289\n",
      "val   Loss: 0.0275\n",
      "\n",
      "Epoch 261/300\n",
      "----------\n",
      "train Loss: 0.0315\n",
      "val   Loss: 0.0323\n",
      "\n",
      "Epoch 262/300\n",
      "----------\n",
      "train Loss: 0.0304\n",
      "val   Loss: 0.0319\n",
      "\n",
      "Epoch 263/300\n",
      "----------\n",
      "train Loss: 0.0319\n",
      "val   Loss: 0.0307\n",
      "\n",
      "Epoch 264/300\n",
      "----------\n",
      "train Loss: 0.0310\n",
      "val   Loss: 0.0295\n",
      "\n",
      "Epoch 265/300\n",
      "----------\n",
      "train Loss: 0.0277\n",
      "val   Loss: 0.0299\n",
      "\n",
      "Epoch 266/300\n",
      "----------\n",
      "train Loss: 0.0279\n",
      "val   Loss: 0.0307\n",
      "\n",
      "Epoch 267/300\n",
      "----------\n",
      "train Loss: 0.0277\n",
      "val   Loss: 0.0263\n",
      "\n",
      "Epoch 268/300\n",
      "----------\n",
      "train Loss: 0.0301\n",
      "val   Loss: 0.0293\n",
      "\n",
      "Epoch 269/300\n",
      "----------\n",
      "train Loss: 0.0297\n",
      "val   Loss: 0.0296\n",
      "\n",
      "Epoch 270/300\n",
      "----------\n",
      "train Loss: 0.0292\n",
      "val   Loss: 0.0286\n",
      "\n",
      "Epoch 271/300\n",
      "----------\n",
      "train Loss: 0.0285\n",
      "val   Loss: 0.0327\n",
      "\n",
      "Epoch 272/300\n",
      "----------\n",
      "train Loss: 0.0278\n",
      "val   Loss: 0.0272\n",
      "\n",
      "Epoch 273/300\n",
      "----------\n",
      "train Loss: 0.0308\n",
      "val   Loss: 0.0288\n",
      "\n",
      "Epoch 274/300\n",
      "----------\n",
      "train Loss: 0.0307\n",
      "val   Loss: 0.0288\n",
      "\n",
      "Epoch 275/300\n",
      "----------\n",
      "train Loss: 0.0283\n",
      "val   Loss: 0.0331\n",
      "\n",
      "Epoch 276/300\n",
      "----------\n",
      "train Loss: 0.0311\n",
      "val   Loss: 0.0296\n",
      "\n",
      "Epoch 277/300\n",
      "----------\n",
      "train Loss: 0.0255\n",
      "val   Loss: 0.0273\n",
      "\n",
      "Epoch 278/300\n",
      "----------\n",
      "train Loss: 0.0272\n",
      "val   Loss: 0.0251\n",
      "\n",
      "Epoch 279/300\n",
      "----------\n",
      "train Loss: 0.0357\n",
      "val   Loss: 0.0300\n",
      "\n",
      "Epoch 280/300\n",
      "----------\n",
      "train Loss: 0.0305\n",
      "val   Loss: 0.0279\n",
      "\n",
      "Epoch 281/300\n",
      "----------\n",
      "train Loss: 0.0245\n",
      "val   Loss: 0.0296\n",
      "\n",
      "Epoch 282/300\n",
      "----------\n",
      "train Loss: 0.0302\n",
      "val   Loss: 0.0297\n",
      "\n",
      "Epoch 283/300\n",
      "----------\n",
      "train Loss: 0.0289\n",
      "val   Loss: 0.0294\n",
      "\n",
      "Epoch 284/300\n",
      "----------\n",
      "train Loss: 0.0249\n",
      "val   Loss: 0.0289\n",
      "\n",
      "Epoch 285/300\n",
      "----------\n",
      "train Loss: 0.0284\n",
      "val   Loss: 0.0273\n",
      "\n",
      "Epoch 286/300\n",
      "----------\n",
      "train Loss: 0.0287\n",
      "val   Loss: 0.0281\n",
      "\n",
      "Epoch 287/300\n",
      "----------\n",
      "train Loss: 0.0267\n",
      "val   Loss: 0.0249\n",
      "\n",
      "Epoch 288/300\n",
      "----------\n",
      "train Loss: 0.0268\n",
      "val   Loss: 0.0283\n",
      "\n",
      "Epoch 289/300\n",
      "----------\n",
      "train Loss: 0.0228\n",
      "val   Loss: 0.0329\n",
      "\n",
      "Epoch 290/300\n",
      "----------\n",
      "train Loss: 0.0280\n",
      "val   Loss: 0.0234\n",
      "\n",
      "Epoch 291/300\n",
      "----------\n",
      "train Loss: 0.0226\n",
      "val   Loss: 0.0254\n",
      "\n",
      "Epoch 292/300\n",
      "----------\n",
      "train Loss: 0.0306\n",
      "val   Loss: 0.0276\n",
      "\n",
      "Epoch 293/300\n",
      "----------\n",
      "train Loss: 0.0283\n",
      "val   Loss: 0.0247\n",
      "\n",
      "Epoch 294/300\n",
      "----------\n",
      "train Loss: 0.0241\n",
      "val   Loss: 0.0325\n",
      "\n",
      "Epoch 295/300\n",
      "----------\n",
      "train Loss: 0.0282\n",
      "val   Loss: 0.0263\n",
      "\n",
      "Epoch 296/300\n",
      "----------\n",
      "train Loss: 0.0274\n",
      "val   Loss: 0.0263\n",
      "\n",
      "Epoch 297/300\n",
      "----------\n",
      "train Loss: 0.0281\n",
      "val   Loss: 0.0256\n",
      "\n",
      "Epoch 298/300\n",
      "----------\n",
      "train Loss: 0.0280\n",
      "val   Loss: 0.0266\n",
      "\n",
      "Epoch 299/300\n",
      "----------\n",
      "train Loss: 0.0236\n",
      "val   Loss: 0.0342\n",
      "\n",
      "Epoch 300/300\n",
      "----------\n",
      "train Loss: 0.0243\n",
      "val   Loss: 0.0258\n",
      "\n",
      "Training complete in 24m 59s\n",
      "Best val Loss: 0.023357 at epoch 289\n"
     ]
    },
    {
     "data": {
      "text/plain": "(VAETrainingModel(\n   (encoder): Encoder(\n     (deep_conv1): TripleConvResidual(\n       (resample): Sequential(\n         (0): Conv2d(3, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n         (1): LayerNorm((256, 240, 32), eps=1e-05, elementwise_affine=True)\n       )\n       (conv1): Conv2d(3, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n       (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n       (conv3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n       (relu): LeakyReLU(negative_slope=0.2)\n       (ln1): LayerNorm((256, 240, 32), eps=1e-05, elementwise_affine=True)\n       (ln2): LayerNorm((256, 240, 32), eps=1e-05, elementwise_affine=True)\n       (ln3): LayerNorm((256, 240, 32), eps=1e-05, elementwise_affine=True)\n     )\n     (deep_conv2): TripleConvResidual(\n       (resample): Sequential(\n         (0): Conv2d(256, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n         (1): LayerNorm((384, 240, 32), eps=1e-05, elementwise_affine=True)\n       )\n       (conv1): Conv2d(256, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n       (conv2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n       (conv3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n       (relu): LeakyReLU(negative_slope=0.2)\n       (ln1): LayerNorm((384, 240, 32), eps=1e-05, elementwise_affine=True)\n       (ln2): LayerNorm((384, 240, 32), eps=1e-05, elementwise_affine=True)\n       (ln3): LayerNorm((384, 240, 32), eps=1e-05, elementwise_affine=True)\n     )\n     (deep_conv3): TripleConvResidual(\n       (resample): Sequential(\n         (0): Conv2d(384, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n         (1): LayerNorm((512, 240, 32), eps=1e-05, elementwise_affine=True)\n       )\n       (conv1): Conv2d(384, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n       (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n       (conv3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n       (relu): LeakyReLU(negative_slope=0.2)\n       (ln1): LayerNorm((512, 240, 32), eps=1e-05, elementwise_affine=True)\n       (ln2): LayerNorm((512, 240, 32), eps=1e-05, elementwise_affine=True)\n       (ln3): LayerNorm((512, 240, 32), eps=1e-05, elementwise_affine=True)\n     )\n     (pool1): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n     (flatten): Flatten(start_dim=1, end_dim=-1)\n     (relu): LeakyReLU(negative_slope=0.2)\n     (linear1): Linear(in_features=913920, out_features=384, bias=True)\n     (norm1): LayerNorm((913920,), eps=1e-05, elementwise_affine=True)\n     (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n     (mean): Linear(in_features=384, out_features=384, bias=True)\n     (var): Linear(in_features=384, out_features=384, bias=True)\n   )\n   (decoder): Decoder(\n     (latent_in): Linear(in_features=384, out_features=384, bias=True)\n     (hidden_layer1): Linear(in_features=384, out_features=913920, bias=True)\n     (unflatten): Unflatten(dim=1, unflattened_size=(512, 119, 15))\n     (unpool): UpsampleAndConv(\n       (refine_conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n       (refine_conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n       (refine_conv3): Conv2d(512, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n       (ln1): LayerNorm((512, 240, 32), eps=1e-05, elementwise_affine=True)\n       (ln2): LayerNorm((512, 240, 32), eps=1e-05, elementwise_affine=True)\n       (ln3): LayerNorm((384, 240, 32), eps=1e-05, elementwise_affine=True)\n       (relu): LeakyReLU(negative_slope=0.2)\n     )\n     (unconv2): TripleConvResidual(\n       (resample): Identity()\n       (conv1): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n       (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n       (conv3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n       (relu): LeakyReLU(negative_slope=0.2)\n       (ln1): LayerNorm((256, 240, 32), eps=1e-05, elementwise_affine=True)\n       (ln2): LayerNorm((256, 240, 32), eps=1e-05, elementwise_affine=True)\n       (ln3): LayerNorm((256, 240, 32), eps=1e-05, elementwise_affine=True)\n     )\n     (unconv1): TripleConvResidual(\n       (resample): Identity()\n       (conv1): Conv2d(256, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n       (conv2): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n       (conv3): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n       (relu): LeakyReLU(negative_slope=0.2)\n       (ln1): LayerNorm((3, 240, 32), eps=1e-05, elementwise_affine=True)\n       (ln2): LayerNorm((3, 240, 32), eps=1e-05, elementwise_affine=True)\n       (ln3): LayerNorm((3, 240, 32), eps=1e-05, elementwise_affine=True)\n     )\n     (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n     (norm2): LayerNorm((913920,), eps=1e-05, elementwise_affine=True)\n     (norm3): LayerNorm((3, 240, 32), eps=1e-05, elementwise_affine=True)\n     (relu): LeakyReLU(negative_slope=0.2)\n     (sigmoid): Sigmoid()\n   )\n ),\n {'train_loss': [1468.7122802734375,\n   827.7348937988281,\n   413.01727294921875,\n   196.24639129638672,\n   87.92644500732422,\n   63.95350456237793,\n   60.43905258178711,\n   67.25088119506836,\n   71.38947296142578,\n   66.61227798461914,\n   55.562801361083984,\n   43.09706497192383,\n   31.91843032836914,\n   23.26458168029785,\n   16.709189414978027,\n   12.28183364868164,\n   9.991201877593994,\n   8.902843475341797,\n   8.450536251068115,\n   7.651009559631348,\n   6.597362279891968,\n   5.198387384414673,\n   4.16424286365509,\n   3.2371764183044434,\n   2.523270845413208,\n   2.1211369037628174,\n   1.8576592803001404,\n   1.547993004322052,\n   1.2827070355415344,\n   1.064599096775055,\n   0.9242892563343048,\n   0.8271135985851288,\n   0.6737208962440491,\n   0.5810563862323761,\n   0.5108935534954071,\n   0.37627677619457245,\n   0.4284224361181259,\n   0.35394974052906036,\n   0.2950943559408188,\n   0.25438055396080017,\n   0.28615257143974304,\n   0.257039338350296,\n   0.24225874990224838,\n   0.20187853276729584,\n   0.18337510526180267,\n   0.20839938521385193,\n   0.18786261230707169,\n   0.17748092114925385,\n   0.2520145773887634,\n   0.1791171357035637,\n   0.15676309168338776,\n   0.2164243906736374,\n   0.14966337010264397,\n   0.17368410527706146,\n   0.16139905154705048,\n   0.15042787790298462,\n   0.15543965250253677,\n   0.16449051350355148,\n   0.1564573459327221,\n   0.12154852598905563,\n   0.15445591509342194,\n   0.14467446506023407,\n   0.10944577306509018,\n   0.14086301624774933,\n   0.12447492405772209,\n   0.13927112519741058,\n   0.14993371814489365,\n   0.1190972626209259,\n   0.11610229313373566,\n   0.09571922197937965,\n   0.10801492631435394,\n   0.1592855006456375,\n   0.11298177391290665,\n   0.1125144325196743,\n   0.11281093209981918,\n   0.10948825255036354,\n   0.10648087412118912,\n   0.15948091074824333,\n   0.132738895714283,\n   0.10649878159165382,\n   0.11459369212388992,\n   0.14936458319425583,\n   0.10357565805315971,\n   0.0957893505692482,\n   0.1252707690000534,\n   0.12826616317033768,\n   0.1080133281648159,\n   0.1063501425087452,\n   0.10022440925240517,\n   0.08754328265786171,\n   0.08272029832005501,\n   0.05675461329519749,\n   0.09586849808692932,\n   0.08927129581570625,\n   0.0917239561676979,\n   0.08010797575116158,\n   0.11906834691762924,\n   0.07781971618533134,\n   0.10155745595693588,\n   0.1046208031475544,\n   0.0680929608643055,\n   0.10071098059415817,\n   0.07361028902232647,\n   0.07122621312737465,\n   0.06799713522195816,\n   0.09100941568613052,\n   0.08708860725164413,\n   0.06897598505020142,\n   0.08843251317739487,\n   0.06899675168097019,\n   0.06253307312726974,\n   0.0666551161557436,\n   0.08147471398115158,\n   0.061380209401249886,\n   0.09424857050180435,\n   0.08173033036291599,\n   0.06470225192606449,\n   0.05369586683809757,\n   0.059466006234288216,\n   0.054695719853043556,\n   0.07658199220895767,\n   0.07020672410726547,\n   0.07268567010760307,\n   0.0550314262509346,\n   0.05963713116943836,\n   0.09150023758411407,\n   0.07264242693781853,\n   0.05420013889670372,\n   0.06420666351914406,\n   0.05874602496623993,\n   0.05442945100367069,\n   0.0765368863940239,\n   0.06992584839463234,\n   0.05847061425447464,\n   0.09925251454114914,\n   0.07821088656783104,\n   0.06932312063872814,\n   0.055385638028383255,\n   0.06772518903017044,\n   0.06149508245289326,\n   0.053256843239068985,\n   0.055593231692910194,\n   0.0632062777876854,\n   0.06065472587943077,\n   0.06310182437300682,\n   0.06299567222595215,\n   0.0928240492939949,\n   0.07127330265939236,\n   0.052216071635484695,\n   0.05874243564903736,\n   0.04158424399793148,\n   0.04826478101313114,\n   0.05543539859354496,\n   0.053966255858540535,\n   0.06100503169000149,\n   0.05997742712497711,\n   0.061458270996809006,\n   0.0636497475206852,\n   0.05241369642317295,\n   0.04997871443629265,\n   0.0572343859821558,\n   0.05415597930550575,\n   0.040186911821365356,\n   0.05829850770533085,\n   0.0528640802949667,\n   0.048848903737962246,\n   0.0677480399608612,\n   0.0650484636425972,\n   0.05101627670228481,\n   0.055054839700460434,\n   0.040628764778375626,\n   0.055092526599764824,\n   0.05399097874760628,\n   0.04459371417760849,\n   0.048528507351875305,\n   0.04676754213869572,\n   0.045981330797076225,\n   0.052623163908720016,\n   0.04046863503754139,\n   0.0481648538261652,\n   0.040648698806762695,\n   0.05426281690597534,\n   0.04330267384648323,\n   0.04533490166068077,\n   0.04358970932662487,\n   0.044788168743252754,\n   0.04819114878773689,\n   0.03719099052250385,\n   0.04410341940820217,\n   0.0562019944190979,\n   0.03957169130444527,\n   0.05327668972313404,\n   0.047377604991197586,\n   0.04424576461315155,\n   0.03893819451332092,\n   0.04441095516085625,\n   0.044562725350260735,\n   0.04498717188835144,\n   0.040892649441957474,\n   0.04275010898709297,\n   0.042236436158418655,\n   0.03698820248246193,\n   0.03657993674278259,\n   0.03529575280845165,\n   0.036915174685418606,\n   0.035738278180360794,\n   0.03745372034609318,\n   0.03694094158709049,\n   0.0373986791819334,\n   0.04123908467590809,\n   0.037047253921628,\n   0.030901470221579075,\n   0.036905646324157715,\n   0.02797599509358406,\n   0.029073458164930344,\n   0.039903389289975166,\n   0.04044363833963871,\n   0.03538035787642002,\n   0.0342329666018486,\n   0.040380602702498436,\n   0.03519715741276741,\n   0.03935542330145836,\n   0.03442750126123428,\n   0.03233031090348959,\n   0.03282996080815792,\n   0.027516493573784828,\n   0.031843421049416065,\n   0.034727444872260094,\n   0.035332655534148216,\n   0.030573911033570766,\n   0.04169640317559242,\n   0.03351760283112526,\n   0.035566605627536774,\n   0.03520415723323822,\n   0.03503734804689884,\n   0.033388057723641396,\n   0.03623158857226372,\n   0.033911602571606636,\n   0.033517918549478054,\n   0.03248357307165861,\n   0.03762420080602169,\n   0.02986834943294525,\n   0.02787645533680916,\n   0.033779848366975784,\n   0.03337095119059086,\n   0.03477567806839943,\n   0.033299870789051056,\n   0.031852537766098976,\n   0.02955448441207409,\n   0.03457514941692352,\n   0.032664960250258446,\n   0.028275770135223866,\n   0.030920693650841713,\n   0.03045573364943266,\n   0.029417522251605988,\n   0.0280090169981122,\n   0.03245986346155405,\n   0.02733699232339859,\n   0.03061598725616932,\n   0.028887280263006687,\n   0.03145717643201351,\n   0.03039007354527712,\n   0.03193254396319389,\n   0.030987726524472237,\n   0.027678238227963448,\n   0.02788015641272068,\n   0.027672821655869484,\n   0.03011615201830864,\n   0.0297079561278224,\n   0.029249937273561954,\n   0.028480129316449165,\n   0.027759486809372902,\n   0.030781522393226624,\n   0.03072853945195675,\n   0.028327738866209984,\n   0.031115415506064892,\n   0.025517068803310394,\n   0.027179092168807983,\n   0.03574490826576948,\n   0.030495936051011086,\n   0.0244730357080698,\n   0.030249882489442825,\n   0.02894742041826248,\n   0.02485900465399027,\n   0.028377531096339226,\n   0.028738481923937798,\n   0.0266991900280118,\n   0.026820790953934193,\n   0.022840787656605244,\n   0.02795120421797037,\n   0.02255337219685316,\n   0.03060934692621231,\n   0.028259812854230404,\n   0.024078491143882275,\n   0.028222953900694847,\n   0.027429800480604172,\n   0.028132536448538303,\n   0.028036904521286488,\n   0.023619042709469795,\n   0.024333304725587368],\n  'val_loss': [962.4921875,\n   478.4033660888672,\n   238.3998565673828,\n   101.64181518554688,\n   64.3329849243164,\n   58.97402763366699,\n   64.75477600097656,\n   71.30780029296875,\n   68.96140670776367,\n   58.73443794250488,\n   46.02630043029785,\n   34.75487518310547,\n   25.09314250946045,\n   18.240221977233887,\n   13.251536846160889,\n   10.30938720703125,\n   9.122478485107422,\n   8.595145225524902,\n   7.857642650604248,\n   6.877166748046875,\n   5.5744781494140625,\n   4.38216495513916,\n   3.282485842704773,\n   2.730649471282959,\n   2.1743204593658447,\n   1.820187270641327,\n   1.541491687297821,\n   1.2794492840766907,\n   1.1654304265975952,\n   0.9634588658809662,\n   0.8190003037452698,\n   0.712094634771347,\n   0.6364180147647858,\n   0.5042027235031128,\n   0.5114106684923172,\n   0.4574175029993057,\n   0.44086018204689026,\n   0.3530036062002182,\n   0.3319595158100128,\n   0.2838001996278763,\n   0.2822355777025223,\n   0.21820108592510223,\n   0.21080467849969864,\n   0.19055960327386856,\n   0.19566547870635986,\n   0.1358923241496086,\n   0.139068603515625,\n   0.1851208731532097,\n   0.19310270249843597,\n   0.15611522644758224,\n   0.2037031129002571,\n   0.13316145166754723,\n   0.16634684056043625,\n   0.16231123358011246,\n   0.17244669049978256,\n   0.14629701524972916,\n   0.14421609044075012,\n   0.12355484440922737,\n   0.14622535184025764,\n   0.10868953540921211,\n   0.15966758877038956,\n   0.15785636007785797,\n   0.10602621734142303,\n   0.09290683269500732,\n   0.12566224485635757,\n   0.1728927567601204,\n   0.0851004458963871,\n   0.12062771618366241,\n   0.0997149609029293,\n   0.09539195150136948,\n   0.09664632380008698,\n   0.12733833119273186,\n   0.1056039035320282,\n   0.09265471994876862,\n   0.10532376542687416,\n   0.13389206677675247,\n   0.10400528833270073,\n   0.10841216892004013,\n   0.11370361223816872,\n   0.10656118765473366,\n   0.0820385292172432,\n   0.1384737715125084,\n   0.09854333847761154,\n   0.09116414561867714,\n   0.13584301620721817,\n   0.10988044366240501,\n   0.09886681288480759,\n   0.0985107310116291,\n   0.1031339205801487,\n   0.10116921737790108,\n   0.07257001101970673,\n   0.10162107646465302,\n   0.10327788442373276,\n   0.08243753388524055,\n   0.09288228675723076,\n   0.09290876984596252,\n   0.09916509687900543,\n   0.08726190403103828,\n   0.06406589969992638,\n   0.08707655593752861,\n   0.08307705260813236,\n   0.08321892470121384,\n   0.06958382204174995,\n   0.09905556961894035,\n   0.07427022978663445,\n   0.07901322841644287,\n   0.07112065702676773,\n   0.08859967067837715,\n   0.05272580124437809,\n   0.08392252400517464,\n   0.08477909490466118,\n   0.06729410775005817,\n   0.08356490358710289,\n   0.08321776241064072,\n   0.063035748898983,\n   0.0952954888343811,\n   0.06441285088658333,\n   0.08338661678135395,\n   0.05848390981554985,\n   0.08070286363363266,\n   0.07117942348122597,\n   0.05929754488170147,\n   0.06699858978390694,\n   0.0716061219573021,\n   0.06805717200040817,\n   0.06761643476784229,\n   0.07706918194890022,\n   0.06007014214992523,\n   0.08734013885259628,\n   0.06077765300869942,\n   0.07047993317246437,\n   0.08294214680790901,\n   0.06032469868659973,\n   0.0710296481847763,\n   0.06515760347247124,\n   0.06395890936255455,\n   0.052448026835918427,\n   0.058779120445251465,\n   0.05285864509642124,\n   0.054676247760653496,\n   0.059962039813399315,\n   0.055439529940485954,\n   0.05082501657307148,\n   0.06030185520648956,\n   0.05752948112785816,\n   0.061401356011629105,\n   0.06297177076339722,\n   0.06750937923789024,\n   0.042694324627518654,\n   0.055202966555953026,\n   0.06269195675849915,\n   0.05144857428967953,\n   0.06274618580937386,\n   0.05672971345484257,\n   0.06138058751821518,\n   0.05754443258047104,\n   0.06513890996575356,\n   0.04513728618621826,\n   0.05979348532855511,\n   0.037967484444379807,\n   0.04246878996491432,\n   0.06287873536348343,\n   0.041234590113162994,\n   0.05020608566701412,\n   0.051001690328121185,\n   0.06580914929509163,\n   0.05206020548939705,\n   0.05632594972848892,\n   0.04907683841884136,\n   0.05064599588513374,\n   0.04678712971508503,\n   0.04009380005300045,\n   0.0385437086224556,\n   0.053320784121751785,\n   0.039906902238726616,\n   0.04810377582907677,\n   0.039507851004600525,\n   0.04356443136930466,\n   0.05308363027870655,\n   0.044317640364170074,\n   0.03918934985995293,\n   0.044362789019942284,\n   0.0504746213555336,\n   0.05016278102993965,\n   0.04081639368087053,\n   0.04292486980557442,\n   0.03610829822719097,\n   0.05231824889779091,\n   0.03961748443543911,\n   0.03681221418082714,\n   0.03998531587421894,\n   0.05146486684679985,\n   0.038977090269327164,\n   0.06057364493608475,\n   0.04464326240122318,\n   0.04615269787609577,\n   0.050870975479483604,\n   0.04555422067642212,\n   0.049053529277443886,\n   0.03911476582288742,\n   0.03811260312795639,\n   0.03482511080801487,\n   0.052289992570877075,\n   0.041491514071822166,\n   0.03099525161087513,\n   0.04099901765584946,\n   0.039675816893577576,\n   0.041873399168252945,\n   0.035108596086502075,\n   0.03229186497628689,\n   0.04017648100852966,\n   0.05025751702487469,\n   0.037678034976124763,\n   0.03741948492825031,\n   0.044596077874302864,\n   0.03727229684591293,\n   0.0407691840082407,\n   0.033680351451039314,\n   0.03312627039849758,\n   0.035590123385190964,\n   0.035716382786631584,\n   0.0384440291672945,\n   0.03866017796099186,\n   0.03557370603084564,\n   0.02924287971109152,\n   0.03754815272986889,\n   0.03286844491958618,\n   0.03558163531124592,\n   0.035265445709228516,\n   0.04137768782675266,\n   0.03458718955516815,\n   0.03012942522764206,\n   0.03110150434076786,\n   0.028644355945289135,\n   0.030155440792441368,\n   0.032975430600345135,\n   0.03710060566663742,\n   0.03516765125095844,\n   0.03532951697707176,\n   0.027190311811864376,\n   0.03368314355611801,\n   0.028917277231812477,\n   0.031494954600930214,\n   0.028846598230302334,\n   0.03069231938570738,\n   0.028276903554797173,\n   0.030646173283457756,\n   0.029548445716500282,\n   0.02581293322145939,\n   0.032924058847129345,\n   0.029824628494679928,\n   0.036414542235434055,\n   0.029412588104605675,\n   0.03036678396165371,\n   0.0322946272790432,\n   0.026480517350137234,\n   0.02994944341480732,\n   0.031149685382843018,\n   0.0286243986338377,\n   0.027462816797196865,\n   0.03225554618984461,\n   0.03190709091722965,\n   0.03072235733270645,\n   0.02950988058000803,\n   0.02985759824514389,\n   0.030698658898472786,\n   0.026295811869204044,\n   0.029299039393663406,\n   0.029572399333119392,\n   0.0285946661606431,\n   0.03269489202648401,\n   0.027179394848644733,\n   0.02879229001700878,\n   0.028821243904531002,\n   0.03311486914753914,\n   0.02957087755203247,\n   0.027280790731310844,\n   0.02512744441628456,\n   0.029950307682156563,\n   0.02787778154015541,\n   0.02955143339931965,\n   0.029743291437625885,\n   0.02944996766746044,\n   0.02891756035387516,\n   0.027344423346221447,\n   0.02810505870729685,\n   0.02492236439138651,\n   0.0282973013818264,\n   0.03288482502102852,\n   0.023357395082712173,\n   0.025391883216798306,\n   0.027562178671360016,\n   0.024654995650053024,\n   0.03248406760394573,\n   0.0262999776750803,\n   0.026334320195019245,\n   0.025605338625609875,\n   0.02663538884371519,\n   0.03423570655286312,\n   0.02576317358762026]})"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EPOCHS = 300\n",
    "#train_model(training_model, get_batch, cost_function, optimizer_, scheduler_, EPOCHS)\n",
    "train_model(training_model, get_batch, cost_function, optimizer_, None, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7cddb3687042bf9",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-11T15:09:32.824024Z",
     "start_time": "2024-01-11T15:09:32.590073Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "# sample images from the latent space\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "\n",
    "def stitch_and_show_images(x):\n",
    "    \"\"\"\n",
    "    Stitch image slices horizontally and show the resulting single image.\n",
    "\n",
    "    :param x: Tensor containing the image slices.\n",
    "    :param slice_shape: Shape of each slice (channels, height, width).\n",
    "    \"\"\"\n",
    "    x = x.view(-1, *slice_shape)  # Reshape\n",
    "    x = x.permute(0, 2, 3, 1)     # Reorder dimensions for plotting\n",
    "\n",
    "    # Concatenate all slices horizontally\n",
    "    stitched_image = torch.cat(tuple(x), dim=1)\n",
    "    \n",
    "    # Calculate the full size for displaying\n",
    "    full_width = stitched_image.shape[1]\n",
    "    full_height = stitched_image.shape[0]\n",
    "    dpi = 80  # Adjust dpi to your screen for accurate sizing (default is usually 80 or 100)\n",
    "    figsize = full_width / dpi, full_height / dpi\n",
    "    \n",
    "    # Convert to numpy and plot\n",
    "    fig = plt.figure(figsize=figsize, dpi=dpi)\n",
    "    plt.imshow(stitched_image.cpu().numpy())\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5bd387098967530f",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-11T15:09:33.587700Z",
     "start_time": "2024-01-11T15:09:33.569316Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m2\u001B[39m):\n\u001B[0;32m----> 2\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[1;32m      3\u001B[0m         noise \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mrandn(SLICES, latent_size)\u001B[38;5;241m.\u001B[39mto(DEVICE)\n\u001B[1;32m      4\u001B[0m         generated_images \u001B[38;5;241m=\u001B[39m training_model\u001B[38;5;241m.\u001B[39mdecoder(noise)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "for _ in range(2):\n",
    "    with torch.no_grad():\n",
    "        noise = torch.randn(SLICES, latent_size).to(DEVICE)\n",
    "        generated_images = training_model.decoder(noise)\n",
    "        stitch_and_show_images(generated_images)    "
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-11T03:33:49.348066Z",
     "start_time": "2024-01-11T03:33:49.347135Z"
    }
   },
   "id": "5ff05026a580b28b",
   "execution_count": 11
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
